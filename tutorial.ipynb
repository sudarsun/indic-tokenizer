{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2dc9d466",
   "metadata": {},
   "source": [
    "##  Demonstration of Grapheme enabled sub-tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "305c1e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gdown\n",
    "\n",
    "import gdown\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35bbfeee",
   "metadata": {},
   "source": [
    "### Let's load the text file (Tamil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65105337",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's down the Tamil example text file from gdrive\n",
    "PATH = \"./tamil.txt\"\n",
    "if not os.path.isfile(\"./tamil.txt\"):\n",
    "    path = gdown.download(url=\"https://drive.google.com/file/d/1-065EvxQepsQWTpDbdAfgPw0kkiwfMcR/view?usp=drive_link\", output=PATH, fuzzy=True)\n",
    "    PATH = path\n",
    "\n",
    "LANG = \"ta\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de86206e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load up the text file.\n",
    "with open(PATH, \"r\") as file:\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    size = len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d58752c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "்கும் மேலதிக வருமானம் உண்மையிலேயே கண்டிக்கத்தக்கதாகும் , மேலும் இந்த மேலதிக வருமானத்திற்கு எதிராக தொழில்முறை நடைமுறை மற்றும் வரிகளை எவ்வாறு சரி செய்யலாம் என நாங்கள் தொடர்ந்து ஆராய்ந்து வருகிறோம் என\n"
     ]
    }
   ],
   "source": [
    "# view a small piece of the text document.\n",
    "LEN = 200\n",
    "pos = np.random.randint(0, size-LEN)\n",
    "print(text[pos:pos+LEN])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c878bbb7",
   "metadata": {},
   "source": [
    "### Let's encode the complex unicode rendering into singular unicode per grapheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd1ece51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indic Unicode Mapper maps the sequence of unicode that constitute a grapheme \n",
    "# into a singular unicode in the 0xE00X range.\n",
    "from indic_unicode_mapper import IndicUnicodeMapper\n",
    "mapper = IndicUnicodeMapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6e691f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode the graphemes.\n",
    "encoded_text = mapper.encode(text=text, lang=LANG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a215e10c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ue075\\ue1a2 \\ue1a5\\ue09b\\ue077\\ue10e எ\\ue0e0\\ue146\\ue10e கவ\\ue161 \\ue00b\\ue09b\\ue075\\ue12f\\ue0c9தன\\ue13c .\\n\\ue0b9\\ue0e0 \\ue0e8\\ue12d\\ue1a0\\ue0d7 ப\\ue153\\ue144 அலச\\ue1a0\\ue16a\\ue161 \\ue0b9\\ue0e0 அ\\ue0c9த \\ue1a0ஷய\\ue0b2\\ue0a9 வச\\ue0a3\\ue115க எ\\ue077\\ue0b2\\ue0a5\\ue011 \\ue00b\\ue09b\\ue07a\\ue0e0 ஏ\\ue0d5\\ue0e0\\ue143\\ue16a அ\\ue0a5 ஏ\\ue153கன\\ue1a5 \\ue102\\ue0e0\\ue146 ம\\ue08c\\ue0bfர \\ue0a3\\ue133\\ue0f7பட\\ue10e ம\\ue153\\ue146\\ue10e \\ue0b9\\ue0e0 அத\\ue0d7\\ue03f \\ue035\\ue13c\\ue0b2\\ue0a3\\ue12f\\ue0c9\\ue0a2\\ue16a அ\\ue0a5 \\ue0ffக \\ue0bb\\ue09bட\\ue0a2க இ\\ue12f\\ue011\\ue004\\ue10e .\\nஇ\\ue0b2த\\ue008ய \\ue101\\ue14a\\ue116\\ue16a உலக ம\\ue011க\\ue181 க\\ue12f\\ue0b2\\ue0a3\\ue153\\ue004 அச\\ue084\\ue07b \\ue001\\ue084\\ue077வ\\ue0a5\\ue10e , \\ue0ff\\ue12fக\\ue0b2தன\\ue0feன '"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the encode text will have unicodes in the 0xE0XX range.\n",
    "pos = np.random.randint(0, len(encoded_text)-LEN)\n",
    "encoded_text[pos:pos+LEN]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3624dac1",
   "metadata": {},
   "source": [
    "### Let's now learn a BERT Tokenizer model on the encoded dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a07b07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the BERT WPE tokenizer module.\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "\n",
    "cls_token = \"[cls]\"\n",
    "sep_token = \"[sep]\"\n",
    "mask_token = \"[mask]\"\n",
    "pad_token = \"[pad]\"\n",
    "unk_token = \"[unk]\"\n",
    "spl_tokens = [\"[unk]\", \"[sep]\", \"[mask]\", \"[cls]\", \"[pad]\"]  # special tokens\n",
    "tokenizer = BertWordPieceTokenizer(clean_text=False, \n",
    "                                   handle_chinese_chars=True, \n",
    "                                   strip_accents=False,\n",
    "                                   lowercase=False,\n",
    "                                   sep_token=sep_token, unk_token=unk_token, \n",
    "                                   mask_token=mask_token, cls_token=cls_token, pad_token=pad_token)\n",
    "\n",
    "# setup the Vocabulary size requirement\n",
    "VOCAB_SIZE = 3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29434b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need a temporary folder to keep the encoded file(s) there.\n",
    "tmpdir = f\"/tmp/mapped-{LANG}\"\n",
    "os.makedirs(tmpdir, exist_ok=True)\n",
    "\n",
    "with open(tmpdir + \"/\" + PATH, \"w\") as ofile:\n",
    "    ofile.write(encoded_text)\n",
    "    ofile.close()\n",
    "\n",
    "# add our encoded text file to the array of paths.\n",
    "files = [tmpdir + \"/\" + PATH]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72b0ca5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# train the algorithm\n",
    "tokenizer.train(files=files, vocab_size=VOCAB_SIZE, min_frequency=2,\n",
    "                limit_alphabet=512, wordpieces_prefix='##',\n",
    "                special_tokens=spl_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d95d7ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the tokenization model.\n",
    "# this line should create a file with the name f\"{LANG}-vocab.txt\"\n",
    "tokenizer.save_model('.', LANG)\n",
    "TOKENIZER_MODEL = f\"{LANG}-vocab.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e31425e",
   "metadata": {},
   "source": [
    "#### If you open the text file, you will see a lot of gibberish as the words are in encoded form.  Let's display a human understandable version side-by-side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "986cea59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's load from the model file.\n",
    "with open(TOKENIZER_MODEL, \"r\") as vfile:\n",
    "    lines = vfile.readlines()\n",
    "    vfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9dab7dd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 3000 vocab items.\n",
      "Encoded Strings      \tOriginal Strings   \n",
      "==================== \t===================\n",
      "##பள            \t##ப்பட்டுள்ள\n",
      "ழ                \tகுழந்தை   \n",
      "ச                  \tசம்       \n",
      "##கக               \t##கமாக    \n",
      "##                \t##ட்ஸ்    \n",
      "##ச                \t##சம்     \n",
      "எ                 \tஎழுதி     \n",
      "ல                \tபாலஸ்தீ   \n",
      "அவ               \tஅவனுக்கு  \n",
      "ன                \tமுன்னர்   \n"
     ]
    }
   ],
   "source": [
    "nlines = len(lines)\n",
    "print(f\"found {nlines} vocab items.\")\n",
    "# we are drawing the lines from the latter half, \n",
    "# as the tokens are longer there.\n",
    "lpos = np.random.randint(0, nlines-5)\n",
    "\n",
    "print(\"Encoded Strings\".ljust(20,' '), \"\\tOriginal Strings\".ljust(20, ' '))\n",
    "print(\"\".ljust(20,'='), \"\\t\".ljust(20, '='))\n",
    "for l in range(lpos, lpos+10):\n",
    "    print(\"%s\\t%s\" % (lines[l].rstrip().ljust(20,' '), mapper.decode(lines[l]).rstrip().ljust(10, ' ')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6585fe09",
   "metadata": {},
   "source": [
    "### Now, let's tokenize the text using the grapheme enabled WPE tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cc593550",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the tokenizer model that inherently does the unicode encoding/decoding.\n",
    "from indic_bert_tokenizer import IndicBertWordPieceTokenizer\n",
    "tokenizer = IndicBertWordPieceTokenizer(model_path=TOKENIZER_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "83be106f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# just use a random text here.\n",
    "test_text = \"ஆனால் மொத்த ஊக்கப் பொதியின் சிறிய அளவு வரி வெட்டுக்கள் மற்றும் செலவின அதிகரிப்புக்கள் இணைந்தது அரசாங்கத்தின் பிற்போக்குத்தன்மையை காட்டும் நடவடிக்கை அல்ல\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4d5fd62f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoding(num_tokens=33, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# encode the input text into token encodings.\n",
    "toks = tokenizer.encode(test_text)\n",
    "toks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "34be8abb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 889, 2450, 49, 672, 368, 214, 370, 739, 1968, 2402, 1943, 2925, 791, 711, 1395, 416, 412, 1404, 1489, 1234, 795, 1640, 206, 2173, 668, 2630, 456, 1625, 359, 901, 888, 1]\n"
     ]
    }
   ],
   "source": [
    "# list to token ids.\n",
    "print(toks.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "425786bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[cls]', 'ஆ\\ue0d0\\ue16a', '\\ue108\\ue0b2த', 'ஊ', '##\\ue011க', '##\\ue0f7', '\\ue0f1', '##\\ue0a3', '##\\ue116\\ue0e0', '\\ue030\\ue144ய', 'அள\\ue1a2', 'வ\\ue12d', '\\ue1a4\\ue084\\ue077', '##\\ue011க\\ue181', 'ம\\ue153\\ue146\\ue10e', '\\ue034ல', '##\\ue1a0', '##ன', 'அ\\ue0a3க\\ue12d', '##\\ue0f7\\ue0ea\\ue011க\\ue181', 'இ\\ue092', '##\\ue0c9த\\ue0a5', 'அர\\ue02f\\ue028க\\ue0b2\\ue0a3\\ue0e0', '\\ue0e8', '##\\ue153\\ue0f9', '##\\ue011\\ue004', '##\\ue0b2த\\ue0e0\\ue105', '##\\ue11c', '\\ue001\\ue084\\ue077', '##\\ue10e', 'நடவ\\ue075\\ue011\\ue008', 'அ\\ue16aல', '[sep]']\n"
     ]
    }
   ],
   "source": [
    "# list the token string (in encoded format)\n",
    "print(toks.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "db533a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[cls]', 'ஆனால்', 'மொத்த', 'ஊ', '##க்க', '##ப்', 'பொ', '##தி', '##யின்', 'சிறிய', 'அளவு', 'வரி', 'வெட்டு', '##க்கள்', 'மற்றும்', 'செல', '##வி', '##ன', 'அதிகரி', '##ப்புக்கள்', 'இணை', '##ந்தது', 'அரசாங்கத்தின்', 'பி', '##ற்போ', '##க்கு', '##த்தன்மை', '##யை', 'காட்டு', '##ம்', 'நடவடிக்கை', 'அல்ல', '[sep]']\n"
     ]
    }
   ],
   "source": [
    "print([tokenizer.decode_string(tok) for tok in toks.tokens])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a024e8ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ஆனால் மொத்த ஊக்கப் பொதியின் சிறிய அளவு வரி வெட்டுக்கள் மற்றும் செலவின அதிகரிப்புக்கள் இணைந்தது அரசாங்கத்தின் பிற்போக்குத்தன்மையை காட்டும் நடவடிக்கை அல்ல'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the decoded string from the tokenizer ids.\n",
    "tokenizer.decode(toks.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2b9d98e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# also check if the input and the decoded string are matching\n",
    "assert(test_text == tokenizer.decode(toks.ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31bd2ccb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tamil",
   "language": "python",
   "name": "tamil"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
