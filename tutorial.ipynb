{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2dc9d466",
   "metadata": {},
   "source": [
    "##  Demonstration of Grapheme enabled sub-tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "305c1e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gdown\n",
    "\n",
    "import gdown\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35bbfeee",
   "metadata": {},
   "source": [
    "### Let's load the text file (Tamil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65105337",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1-065EvxQepsQWTpDbdAfgPw0kkiwfMcR\n",
      "To: /home/sudarsun/github/indic-tokenizer/tamil.txt\n",
      "100%|██████████| 18.8M/18.8M [00:01<00:00, 10.0MB/s]\n"
     ]
    }
   ],
   "source": [
    "# let's down the Tamil example text file from gdrive\n",
    "PATH = \"./tamil.txt\"\n",
    "if not os.path.isfile(\"./tamil.txt\"):\n",
    "    path = gdown.download(url=\"https://drive.google.com/file/d/1-065EvxQepsQWTpDbdAfgPw0kkiwfMcR/view?usp=drive_link\", output=PATH, fuzzy=True)\n",
    "    PATH = path\n",
    "\n",
    "LANG = \"ta\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de86206e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load up the text file.\n",
    "with open(PATH, \"r\") as file:\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    size = len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d58752c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ளுக்கு எதிரே இருக்கிறதைப் பழுதுபார்த்துக் கட்டினார்கள் .\n",
      "ஷாஜகான் , அல்லி அர்ஜூனா , காதல் கிறுக்கன் படங்களுக்கு பிறகு காணாமல் போன ரிச்சா பலோட் திரும்பி வந்திருக்கிறார் .\n",
      "மேலும் மிக கஷ்டப்படுபவர்களுக்கு\n"
     ]
    }
   ],
   "source": [
    "# view a small piece of the text document.\n",
    "LEN = 200\n",
    "pos = np.random.randint(0, size-LEN)\n",
    "print(text[pos:pos+LEN])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c878bbb7",
   "metadata": {},
   "source": [
    "### Let's encode the complex unicode rendering into singular unicode per grapheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd1ece51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indic Unicode Mapper maps the sequence of unicode that constitute a grapheme \n",
    "# into a singular unicode in the 0xE00X range.\n",
    "from indic_unicode_mapper import IndicUnicodeMapper\n",
    "mapper = IndicUnicodeMapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6e691f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode the graphemes.\n",
    "encoded_text = mapper.encode(text=text, lang=LANG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a215e10c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' ஆ\\ue09b\\ue077 \\ue1a6\\ue001\\ue030\\ue116\\ue16a த\\ue153\\ue001\\ue15bக\\ue0feக\\ue0b2 த\\ue181\\ue172\\ue0f7 \\ue0f9ட\\ue0f7ப\\ue084ட\\ue0a5 .\\n\\ue0e8\\ue12c\\ue001ர \\ue19fச\\ue15b\\ue0e0 \\ue0aa\\ue028\\ue004\\ue0a3\\ue133 இள\\ue0bbல \\ue0bd\\ue15a\\ue15d\\ue10e இர\\ue0b2\\ue0a2\\ue10eபர\\ue0bd\\ue15a\\ue15d\\ue10e \\ue030வ\\ue0f7\\ue0ea\\ue0bd\\ue15a\\ue15d\\ue10e \\ue0a3\\ue12d\\ue0b2த \\ue103\\ue16a\\ue15bய ப\\ue06d\\ue032\\ue0bd\\ue15a\\ue15d\\ue10e \\ue034\\ue125ய\\ue0f7ப\\ue084ட \\ue1a0\\ue030\\ue0b2\\ue0a3ர\\ue0b2\\ue0a9ய\\ue16a \\ue1a5\\ue161\\ue115\\ue116\\ue12f\\ue0c9த\\ue0a5; அ\\ue0a3\\ue0e0 \\ue0bbள\\ue10e இ\\ue12fப\\ue0a5 \\ue101ழ\\ue10e , அ\\ue0a3\\ue0e0 அகல\\ue101\\ue10e உயர\\ue101\\ue10e \\ue0e8\\ue12c\\ue001ர\\ue0b2\\ue0a3\\ue0e0 \\ue0aa\\ue028\\ue004\\ue0a3\\ue133க\\ue174\\ue011\\ue004\\ue03f ச\\ue12d\\ue115\\ue125 ஐ\\ue0c9\\ue0a5 \\ue101ழ\\ue10e .\\nஎ\\ue0f7\\ue0f9'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the encode text will have unicodes in the 0xE0XX range.\n",
    "pos = np.random.randint(0, len(encoded_text)-LEN)\n",
    "encoded_text[pos:pos+LEN]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3624dac1",
   "metadata": {},
   "source": [
    "### Let's now learn a BERT Tokenizer model on the encoded dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a07b07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the BERT WPE tokenizer module.\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "\n",
    "cls_token = \"[cls]\"\n",
    "sep_token = \"[sep]\"\n",
    "mask_token = \"[mask]\"\n",
    "pad_token = \"[pad]\"\n",
    "unk_token = \"[unk]\"\n",
    "spl_tokens = [\"[unk]\", \"[sep]\", \"[mask]\", \"[cls]\", \"[pad]\"]  # special tokens\n",
    "tokenizer = BertWordPieceTokenizer(clean_text=False, \n",
    "                                   handle_chinese_chars=True, \n",
    "                                   strip_accents=False,\n",
    "                                   lowercase=False,\n",
    "                                   sep_token=sep_token, unk_token=unk_token, \n",
    "                                   mask_token=mask_token, cls_token=cls_token, pad_token=pad_token)\n",
    "\n",
    "# setup the Vocabulary size requirement\n",
    "VOCAB_SIZE = 3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "29434b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need a temporary folder to keep the encoded file(s) there.\n",
    "tmpdir = f\"/tmp/mapped-{LANG}\"\n",
    "os.makedirs(tmpdir, exist_ok=True)\n",
    "\n",
    "with open(tmpdir + \"/\" + PATH, \"w\") as ofile:\n",
    "    ofile.write(encoded_text)\n",
    "    ofile.close()\n",
    "\n",
    "# add our encoded text file to the array of paths.\n",
    "files = [tmpdir + \"/\" + PATH]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "72b0ca5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# train the algorithm\n",
    "tokenizer.train(files=files, vocab_size=VOCAB_SIZE, min_frequency=2,\n",
    "                limit_alphabet=512, wordpieces_prefix='##',\n",
    "                special_tokens=spl_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d95d7ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the tokenization model.\n",
    "# this line should create a file with the name f\"{LANG}-vocab.txt\"\n",
    "tokenizer.save_model('.', LANG)\n",
    "TOKENIZER_MODEL = f\"{LANG}-vocab.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e31425e",
   "metadata": {},
   "source": [
    "#### If you open the text file, you will see a lot of gibberish as the words are in encoded form.  Let's display a human understandable version side-by-side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "986cea59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's load from the model file.\n",
    "with open(TOKENIZER_MODEL, \"r\") as vfile:\n",
    "    lines = vfile.readlines()\n",
    "    vfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9dab7dd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 3000 vocab items.\n",
      "Encoded Strings      \tOriginal Strings   \n",
      "==================== \t===================\n",
      "த                \tமுடியாத   \n",
      "                  \tபோய்      \n",
      "##               \t##ற்பாடு  \n",
      "##க                \t##யோக    \n",
      "                \tதேர்ந்தெ  \n",
      "உ               \tஉள்நாட்டு \n",
      "ஐ                 \tஐந்து     \n",
      "##                \t##சாலை    \n",
      "##கட               \t##க்கட    \n",
      "##ன               \t##யானது   \n"
     ]
    }
   ],
   "source": [
    "nlines = len(lines)\n",
    "print(f\"found {nlines} vocab items.\")\n",
    "# we are drawing the lines from the latter half, \n",
    "# as the tokens are longer there.\n",
    "lpos = np.random.randint(0, nlines-5)\n",
    "\n",
    "print(\"Encoded Strings\".ljust(20,' '), \"\\tOriginal Strings\".ljust(20, ' '))\n",
    "print(\"\".ljust(20,'='), \"\\t\".ljust(20, '='))\n",
    "for l in range(lpos, lpos+10):\n",
    "    print(\"%s\\t%s\" % (lines[l].rstrip().ljust(20,' '), mapper.decode(lines[l]).rstrip().ljust(10, ' ')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc61c03b",
   "metadata": {},
   "source": [
    "### From v0.0.3, learning the tokenizer model is a part of the package."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbd1c5f",
   "metadata": {},
   "source": [
    "The model file is typically fixed as `indic-bert-tokenizer-vocab.txt`.  If you need the human readable version, additionally, `indic-bert-tokenizer-vocab.indic.txt` is also created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bca762",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-06-23 12:27:17,260] [INFO] IndicBERTWPETokenizer.build_model Using temporary directory /tmp/tmpp70cbbph for mapped files.\n",
      "[2025-06-23 12:27:17,261] [INFO] IndicBERTWPETokenizer.build_model Processing 1 files for vocabulary building.\n",
      "[2025-06-23 12:27:17,262] [INFO] IndicBERTWPETokenizer.build_model Processing file tamil.txt -> /tmp/tmpp70cbbph/tamil.txt\n",
      "[2025-06-23 12:27:18,833] [INFO] IndicBERTWPETokenizer.build_model Training tokenizer on 1 files with vocab size 3000 and min frequency 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-06-23 12:27:29,394] [INFO] IndicBERTWPETokenizer.build_model Saving tokenizer model to ./indic-bert-tokenizer-vocab.txt\n",
      "[2025-06-23 12:27:29,397] [INFO] IndicBERTWPETokenizer.build_model Creating human readable vocabulary file at ./indic-bert-tokenizer-vocab.indic.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "from indic_bert_tokenizer import IndicBertWordPieceTokenizer\n",
    "tokenizer = IndicBertWordPieceTokenizer.build_model(files = ['tamil.txt'],\n",
    "                                                    vocab_size=VOCAB_SIZE,\n",
    "                                                    model_dir='.',\n",
    "                                                    human_readable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b40668",
   "metadata": {},
   "source": [
    "Let's display the model files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ddc74411",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's load from the model file.\n",
    "with open('indic-bert-tokenizer-vocab.txt', \"r\") as vfile:\n",
    "    lines = [line.strip() for line in vfile.readlines()]\n",
    "    vfile.close()\n",
    "\n",
    "with open('indic-bert-tokenizer-vocab.indic.txt', \"r\") as vfile:\n",
    "    hlines = [line.strip() for line in vfile.readlines()]\n",
    "    vfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0f877bdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ப\\ue07b', 'படை'),\n",
       " ('ம\\ue011க\\ue181', 'மக்கள்'),\n",
       " ('எ\\ue0e0ன', 'என்ன'),\n",
       " ('அ\\ue16aல\\ue0a5', 'அல்லது'),\n",
       " ('\\ue034\\ue125\\ue0a3', 'செய்தி'),\n",
       " ('அவ\\ue0e0', 'அவன்'),\n",
       " ('பய', 'பய'),\n",
       " ('இ\\ue12f\\ue0c9த', 'இருந்த'),\n",
       " ('##\\ue12f\\ue011\\ue002ற\\ue0a5', '##ருக்கிறது'),\n",
       " ('இ\\ue0c9\\ue0a3', 'இந்தி'),\n",
       " ('##\\ue0e0\\ue143', '##ன்றா'),\n",
       " ('##\\ue084\\ue074\\ue13c', '##ட்டார்'),\n",
       " ('##\\ue0a3ப\\ue0a3', '##திபதி'),\n",
       " ('\\ue005\\ue084ட', 'கூட்ட'),\n",
       " ('\\ue004\\ue18b', 'குழு'),\n",
       " ('அ\\ue0a9', 'அதை'),\n",
       " ('\\ue0f1\\ue0a5', 'பொது'),\n",
       " ('##\\ue12f\\ue07bய', '##ருடைய'),\n",
       " ('##த\\ue161', '##தலை'),\n",
       " ('கட\\ue0c9த', 'கடந்த')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(lines[1000:1020], hlines[1000:1020]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6585fe09",
   "metadata": {},
   "source": [
    "### Now, let's tokenize the text using the grapheme enabled WPE tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cc593550",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the tokenizer model that inherently does the unicode encoding/decoding.\n",
    "from indic_bert_tokenizer import IndicBertWordPieceTokenizer\n",
    "tokenizer = IndicBertWordPieceTokenizer(model_path=\"./indic-bert-tokenizer-vocab.txt\") # TOKENIZER_MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "83be106f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# just use a random text here.\n",
    "test_text = \"ஆனால் மொத்த ஊக்கப் பொதியின் சிறிய அளவு வரி வெட்டுக்கள் மற்றும் செலவின அதிகரிப்புக்கள் இணைந்தது அரசாங்கத்தின் பிற்போக்குத்தன்மையை காட்டும் நடவடிக்கை அல்ல\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4d5fd62f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoding(num_tokens=33, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# encode the input text into token encodings.\n",
    "toks = tokenizer.encode(test_text)\n",
    "toks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "34be8abb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 889, 2450, 49, 672, 382, 214, 364, 739, 1968, 2402, 1943, 2925, 791, 711, 1395, 387, 410, 1404, 1489, 1234, 795, 1640, 206, 2173, 668, 2630, 478, 1625, 359, 901, 888, 1]\n"
     ]
    }
   ],
   "source": [
    "# list to token ids.\n",
    "print(toks.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "425786bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[cls]', 'ஆ\\ue0d0\\ue16a', '\\ue108\\ue0b2த', 'ஊ', '##\\ue011க', '##\\ue0f7', '\\ue0f1', '##\\ue0a3', '##\\ue116\\ue0e0', '\\ue030\\ue144ய', 'அள\\ue1a2', 'வ\\ue12d', '\\ue1a4\\ue084\\ue077', '##\\ue011க\\ue181', 'ம\\ue153\\ue146\\ue10e', '\\ue034ல', '##\\ue1a0', '##ன', 'அ\\ue0a3க\\ue12d', '##\\ue0f7\\ue0ea\\ue011க\\ue181', 'இ\\ue092', '##\\ue0c9த\\ue0a5', 'அர\\ue02f\\ue028க\\ue0b2\\ue0a3\\ue0e0', '\\ue0e8', '##\\ue153\\ue0f9', '##\\ue011\\ue004', '##\\ue0b2த\\ue0e0\\ue105', '##\\ue11c', '\\ue001\\ue084\\ue077', '##\\ue10e', 'நடவ\\ue075\\ue011\\ue008', 'அ\\ue16aல', '[sep]']\n"
     ]
    }
   ],
   "source": [
    "# list the token string (in encoded format)\n",
    "print(toks.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "db533a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[cls]', 'ஆனால்', 'மொத்த', 'ஊ', '##க்க', '##ப்', 'பொ', '##தி', '##யின்', 'சிறிய', 'அளவு', 'வரி', 'வெட்டு', '##க்கள்', 'மற்றும்', 'செல', '##வி', '##ன', 'அதிகரி', '##ப்புக்கள்', 'இணை', '##ந்தது', 'அரசாங்கத்தின்', 'பி', '##ற்போ', '##க்கு', '##த்தன்மை', '##யை', 'காட்டு', '##ம்', 'நடவடிக்கை', 'அல்ல', '[sep]']\n"
     ]
    }
   ],
   "source": [
    "print([tokenizer.decode_string(tok) for tok in toks.tokens])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a024e8ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ஆனால் மொத்த ஊக்கப் பொதியின் சிறிய அளவு வரி வெட்டுக்கள் மற்றும் செலவின அதிகரிப்புக்கள் இணைந்தது அரசாங்கத்தின் பிற்போக்குத்தன்மையை காட்டும் நடவடிக்கை அல்ல'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the decoded string from the tokenizer ids.\n",
    "tokenizer.decode(toks.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2b9d98e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# also check if the input and the decoded string are matching\n",
    "assert(test_text == tokenizer.decode(toks.ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f8b16f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "demos",
   "language": "python",
   "name": "demos"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
