{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2dc9d466",
   "metadata": {},
   "source": [
    "##  Demonstration of Grapheme enabled sub-tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "305c1e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gdown\n",
    "\n",
    "import gdown\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35bbfeee",
   "metadata": {},
   "source": [
    "### Let's load the text file (Tamil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65105337",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's down the Tamil example text file from gdrive\n",
    "PATH = \"./tamil.txt\"\n",
    "if not os.path.isfile(\"./tamil.txt\"):\n",
    "    path = gdown.download(url=\"https://drive.google.com/file/d/1-065EvxQepsQWTpDbdAfgPw0kkiwfMcR/view?usp=drive_link\", output=PATH, fuzzy=True)\n",
    "    PATH = path\n",
    "\n",
    "LANG = \"ta\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de86206e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load up the text file.\n",
    "with open(PATH, \"r\") as file:\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    size = len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d58752c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "்கச் செய்தி ஊடகத்தால் கூறப்படுவது போல் நிராகரிக்கப்பட்டு விட்டால் , அதன் விளைவு தவிர்க்க முடியாத வெள்ளைப் பூச்சாகத்தன் இருக்கும்; அப்படித்தான் 911 தாக்குதல்கள் பற்றி பல உத்தியோகபூர்வ விசாரணைகளின் கத\n"
     ]
    }
   ],
   "source": [
    "# view a small piece of the text document.\n",
    "LEN = 200\n",
    "pos = np.random.randint(0, size-LEN)\n",
    "print(text[pos:pos+LEN])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c878bbb7",
   "metadata": {},
   "source": [
    "### Let's encode the complex unicode rendering into singular unicode per grapheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd1ece51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indic Unicode Mapper maps the sequence of unicode that constitute a grapheme \n",
    "# into a singular unicode in the 0xE00X range.\n",
    "from indic_unicode_mapper import IndicUnicodeMapper\n",
    "mapper = IndicUnicodeMapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6e691f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode the graphemes.\n",
    "encoded_text = mapper.encode(text=text, lang=LANG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a215e10c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'எ\\ue0a3\\ue136\\ue15b\\ue0b2\\ue0a5\\ue181ளன\\ue13c; \\ue104\\ue15d\\ue10e அ\\ue103\\ue12d\\ue011\\ue001 \\ue101\\ue1cdர\\ue0f7\\ue0eaட\\ue0e0 \\ue0be\\ue12f\\ue011க\\ue0feன ந\\ue084\\ue0ee\\ue0b2 \\ue0acடர \\ue1a5\\ue09b\\ue077\\ue10e எ\\ue0e0\\ue146\\ue10e , எ\\ue16a\\ue15aவ\\ue153\\ue144\\ue004\\ue10e \\ue104\\ue15aக அவ\\ue12f\\ue07bய ஆ\\ue084\\ue030 ந\\ue10e\\ue0e8\\ue116\\ue12f\\ue011\\ue004\\ue10e \\ue0e7\\ue011\\ue002\\ue1f4\\ue0a2\\ue0d1ய இ\\ue12c\\ue08eவ\\ue0b2\\ue0a5ட\\ue0d3\\ue10e \\ue0be\\ue12f\\ue011க\\ue0feக இ\\ue12f\\ue011க \\ue1a5\\ue09b\\ue077\\ue10e எ\\ue0e0\\ue146\\ue10e \\ue005\\ue144\\ue118\\ue181ளன\\ue13c .\\nஎ\\ue0e0\\ue0d7 \\ue0b9\\ue0d6 இ\\ue028\\ue004 அ\\ue18f\\ue0b2\\ue0a5 வ\\ue0c9\\ue0a5\\ue181\\ue177\\ue0e0 ! .\\nபட\\ue0b2\\ue0a5\\ue011\\ue001ன ட\\ue0f7\\ue0e8\\ue028\\ue004\\ue10e ர\\ue047\\ue0d1\\ue11b \\ue0ed\\ue032\\ue19f\\ue13c என த\\ue115\\ue12d\\ue0f7\\ue0e7'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the encode text will have unicodes in the 0xE0XX range.\n",
    "pos = np.random.randint(0, len(encoded_text)-LEN)\n",
    "encoded_text[pos:pos+LEN]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3624dac1",
   "metadata": {},
   "source": [
    "### Let's now learn a BERT Tokenizer model on the encoded dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a07b07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the BERT WPE tokenizer module.\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "\n",
    "cls_token = \"[cls]\"\n",
    "sep_token = \"[sep]\"\n",
    "mask_token = \"[mask]\"\n",
    "pad_token = \"[pad]\"\n",
    "unk_token = \"[unk]\"\n",
    "spl_tokens = [\"[unk]\", \"[sep]\", \"[mask]\", \"[cls]\", \"[pad]\"]  # special tokens\n",
    "tokenizer = BertWordPieceTokenizer(clean_text=False, \n",
    "                                   handle_chinese_chars=True, \n",
    "                                   strip_accents=False,\n",
    "                                   lowercase=False,\n",
    "                                   sep_token=sep_token, unk_token=unk_token, \n",
    "                                   mask_token=mask_token, cls_token=cls_token, pad_token=pad_token)\n",
    "\n",
    "# setup the Vocabulary size requirement\n",
    "VOCAB_SIZE = 3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29434b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need a temporary folder to keep the encoded file(s) there.\n",
    "tmpdir = f\"/tmp/mapped-{LANG}\"\n",
    "os.makedirs(tmpdir, exist_ok=True)\n",
    "\n",
    "with open(tmpdir + \"/\" + PATH, \"w\") as ofile:\n",
    "    ofile.write(encoded_text)\n",
    "    ofile.close()\n",
    "\n",
    "# add our encoded text file to the array of paths.\n",
    "files = [tmpdir + \"/\" + PATH]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72b0ca5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# train the algorithm\n",
    "tokenizer.train(files=files, vocab_size=VOCAB_SIZE, min_frequency=2,\n",
    "                limit_alphabet=512, wordpieces_prefix='##',\n",
    "                special_tokens=spl_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d95d7ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the tokenization model.\n",
    "# this line should create a file with the name f\"{LANG}-vocab.txt\"\n",
    "tokenizer.save_model('.', LANG)\n",
    "TOKENIZER_MODEL = f\"{LANG}-vocab.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e31425e",
   "metadata": {},
   "source": [
    "#### If you open the text file, you will see a lot of gibberish as the words are in encoded form.  Let's display a human understandable version side-by-side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "986cea59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's load from the model file.\n",
    "with open(TOKENIZER_MODEL, \"r\") as vfile:\n",
    "    lines = vfile.readlines()\n",
    "    vfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9dab7dd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 3000 vocab items.\n",
      "Encoded Strings      \tOriginal Strings   \n",
      "==================== \t===================\n",
      "வறன              \tவருகின்றன \n",
      "அவ               \tஅனைவரும்  \n",
      "ர                \tகுமாரன்   \n",
      "தகப                \tதகப்ப     \n",
      "##கபள          \t##க்கப்பட்டுள்ள\n",
      "கசன               \tகணிசமான   \n",
      "ர                 \tமுரண்     \n",
      "டப              \tசெப்டம்பர்\n",
      "ஆ                 \tஆய்வு     \n",
      "தள                 \tதள்ள      \n"
     ]
    }
   ],
   "source": [
    "nlines = len(lines)\n",
    "print(f\"found {nlines} vocab items.\")\n",
    "# we are drawing the lines from the latter half, \n",
    "# as the tokens are longer there.\n",
    "lpos = np.random.randint(0, nlines-5)\n",
    "\n",
    "print(\"Encoded Strings\".ljust(20,' '), \"\\tOriginal Strings\".ljust(20, ' '))\n",
    "print(\"\".ljust(20,'='), \"\\t\".ljust(20, '='))\n",
    "for l in range(lpos, lpos+10):\n",
    "    print(\"%s\\t%s\" % (lines[l].rstrip().ljust(20,' '), mapper.decode(lines[l]).rstrip().ljust(10, ' ')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6585fe09",
   "metadata": {},
   "source": [
    "### Now, let's tokenize the text using the grapheme enabled WPE tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cc593550",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the tokenizer model that inherently does the unicode encoding/decoding.\n",
    "from indic_bert_tokenizer import IndicBertWordPieceTokenizer\n",
    "tokenizer = IndicBertWordPieceTokenizer(model_path=TOKENIZER_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "83be106f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# just use a random text here.\n",
    "test_text = \"ஆனால் மொத்த ஊக்கப் பொதியின் சிறிய அளவு வரி வெட்டுக்கள் மற்றும் செலவின அதிகரிப்புக்கள் இணைந்தது அரசாங்கத்தின் பிற்போக்குத்தன்மையை காட்டும் நடவடிக்கை அல்ல\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4d5fd62f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoding(num_tokens=33, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# encode the input text into token encodings.\n",
    "toks = tokenizer.encode(test_text)\n",
    "toks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "34be8abb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 889, 2453, 49, 672, 365, 214, 389, 739, 1969, 2404, 1945, 2925, 791, 711, 1394, 383, 408, 1403, 1488, 1233, 795, 1639, 206, 2174, 668, 2631, 439, 1624, 414, 901, 888, 1]\n"
     ]
    }
   ],
   "source": [
    "# list to token ids.\n",
    "print(toks.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "425786bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[cls]', 'ஆ\\ue0d0\\ue16a', '\\ue108\\ue0b2த', 'ஊ', '##\\ue011க', '##\\ue0f7', '\\ue0f1', '##\\ue0a3', '##\\ue116\\ue0e0', '\\ue030\\ue144ய', 'அள\\ue1a2', 'வ\\ue12d', '\\ue1a4\\ue084\\ue077', '##\\ue011க\\ue181', 'ம\\ue153\\ue146\\ue10e', '\\ue034ல', '##\\ue1a0', '##ன', 'அ\\ue0a3க\\ue12d', '##\\ue0f7\\ue0ea\\ue011க\\ue181', 'இ\\ue092', '##\\ue0c9த\\ue0a5', 'அர\\ue02f\\ue028க\\ue0b2\\ue0a3\\ue0e0', '\\ue0e8', '##\\ue153\\ue0f9', '##\\ue011\\ue004', '##\\ue0b2த\\ue0e0\\ue105', '##\\ue11c', '\\ue001\\ue084\\ue077', '##\\ue10e', 'நடவ\\ue075\\ue011\\ue008', 'அ\\ue16aல', '[sep]']\n"
     ]
    }
   ],
   "source": [
    "# list the token string (in encoded format)\n",
    "print(toks.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "db533a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[cls]', 'ஆனால்', 'மொத்த', 'ஊ', '##க்க', '##ப்', 'பொ', '##தி', '##யின்', 'சிறிய', 'அளவு', 'வரி', 'வெட்டு', '##க்கள்', 'மற்றும்', 'செல', '##வி', '##ன', 'அதிகரி', '##ப்புக்கள்', 'இணை', '##ந்தது', 'அரசாங்கத்தின்', 'பி', '##ற்போ', '##க்கு', '##த்தன்மை', '##யை', 'காட்டு', '##ம்', 'நடவடிக்கை', 'அல்ல', '[sep]']\n"
     ]
    }
   ],
   "source": [
    "print([tokenizer.decode_string(tok) for tok in toks.tokens])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a024e8ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ஆனால் மொத்த ஊக்கப் பொதியின் சிறிய அளவு வரி வெட்டுக்கள் மற்றும் செலவின அதிகரிப்புக்கள் இணைந்தது அரசாங்கத்தின் பிற்போக்குத்தன்மையை காட்டும் நடவடிக்கை அல்ல'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the decoded string from the tokenizer ids.\n",
    "tokenizer.decode(toks.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2b9d98e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# also check if the input and the decoded string are matching\n",
    "assert(test_text == tokenizer.decode(toks.ids))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tamil",
   "language": "python",
   "name": "tamil"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
